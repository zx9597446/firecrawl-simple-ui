import os
import requests
import streamlit as st
from dotenv import load_dotenv
from time import sleep

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
API_URL = os.getenv("FIRECRAWL_API_URL", "https://api.firecrawl.dev/v1")
API_KEY = os.getenv("FIRECRAWL_API_KEY")

# åˆå§‹åŒ–sessionçŠ¶æ€
if 'job_id' not in st.session_state:
    st.session_state.job_id = None
if 'results' not in st.session_state:
    st.session_state.results = None

def submit_map_job(url, options):
    """æäº¤URLæ˜ å°„ä»»åŠ¡"""
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "url": url,
        "ignoreSitemap": options["ignore_sitemap"],
        "sitemapOnly": options["sitemap_only"],
        "includeSubdomains": options["include_subdomains"],
        "limit": options["limit"],
    }
    
    # å¯é€‰å‚æ•°
    if options["search"]:
        payload["search"] = options["search"]
    if options["timeout"]:
        payload["timeout"] = options["timeout"]

    try:
        response = requests.post(
            f"{API_URL}/map",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"æäº¤å¤±è´¥: {str(e)}")
        return None

# Streamlitç•Œé¢
st.title("ğŸ—ºï¸ Firecrawl URLæ˜ å°„å·¥å…·")

# é…ç½®é€‰é¡¹
with st.expander("é«˜çº§æ˜ å°„é€‰é¡¹"):
    col1, col2 = st.columns(2)
    with col1:
        ignore_sitemap = st.checkbox("å¿½ç•¥ç«™ç‚¹åœ°å›¾", value=True)
        sitemap_only = st.checkbox("ä»…ç«™ç‚¹åœ°å›¾", value=False)
        include_subdomains = st.checkbox("åŒ…å«å­åŸŸå", value=False)
        
    with col2:
        limit = st.number_input("æœ€å¤§é“¾æ¥æ•°", min_value=1, max_value=5000, value=100)
        timeout = st.number_input("è¶…æ—¶æ—¶é—´(ms)", min_value=0, value=0)
        
    search = st.text_input("æœç´¢æŸ¥è¯¢") 

# URLè¾“å…¥æ¡†
url = st.text_input(
    "è¾“å…¥è¦æ˜ å°„çš„URL",
    placeholder="https://example.com",
    help="è¾“å…¥è¦å‘ç°é“¾æ¥çš„èµ·å§‹URL"
)

# æäº¤æŒ‰é’®
if st.button("å¼€å§‹æ˜ å°„") and url:
    options = {
        "ignore_sitemap": ignore_sitemap,
        "sitemap_only": sitemap_only,
        "include_subdomains": include_subdomains,
        "limit": limit,
        "timeout": timeout if timeout > 0 else None,
        "search": search if search else None,
    }
    
    with st.spinner("æäº¤ä»»åŠ¡ä¸­..."):
        result = submit_map_job(url.strip(), options)
        
    if not result:
        st.error("ä»»åŠ¡æäº¤å¤±è´¥: æ— å“åº”")
    elif result.get("error"):
        st.error(f"ä»»åŠ¡æäº¤å¤±è´¥: {result['error']}")
    elif not result.get("success"):
        st.error(f"ä»»åŠ¡æäº¤å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
    else:
        st.session_state.results = result
        st.success("æ˜ å°„å®Œæˆï¼")

# ç»“æœæ˜¾ç¤ºå’Œä¸‹è½½
if st.session_state.results:
    st.divider()
    st.subheader("æ˜ å°„ç»“æœ")
    
    if not st.session_state.results.get("links"):
        st.warning("æ²¡æœ‰å‘ç°ä»»ä½•é“¾æ¥")
    else:
        links = st.session_state.results["links"]
        st.success(f"å‘ç° {len(links)} ä¸ªé“¾æ¥")
        
        # æ˜¾ç¤ºé“¾æ¥åˆ—è¡¨
        st.dataframe(links, use_container_width=True)
        
        # ä¸‹è½½é“¾æ¥æŒ‰é’®
        combined_links = "\n".join(links)
        st.download_button(
            label="ä¸‹è½½é“¾æ¥åˆ—è¡¨",
            data=combined_links,
            file_name="discovered_links.txt",
            mime="text/plain"
        )
